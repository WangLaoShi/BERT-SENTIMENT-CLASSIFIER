训练速度较慢的主要原因可能有以下几个方面：
1. **批量大小（Batch Size）较小** 🏋️‍♂️  
   - 当前批量大小是 **8**，可以尝试增大，比如 **16 或 32**（前提是显存允许）。
  
2. **使用 `fp16` 进行混合精度训练** ⚡  
   - **自动混合精度训练（AMP）** 可以加速计算并减少显存占用。

3. **数据加载速度较慢** ⏳  
   - **使用 `num_workers` 加速数据加载**，避免 CPU 读取数据成为瓶颈。

4. **优化计算效率** 🔥  
   - **冻结 BERT 的底层权重**，只训练上层分类头。

---

## **🔥 解决方案**
我会 **一步步优化你的代码**，提高训练速度！

### **✅ 方案 1：增大 Batch Size**
**修改 `train_loader` 和 `test_loader` 的 `batch_size=16` 或 `batch_size=32`**（如果显存允许）
```python
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)
```
- **如果显存不足，可以尝试 `batch_size=16`，否则 `32` 更快**。

---

### **✅ 方案 2：启用 `fp16` 混合精度训练**
使用 `torch.cuda.amp` 进行 **自动混合精度（AMP）**，减少 GPU 计算量：
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

def train_model(model, train_loader, test_loader, optimizer, loss_fn, epochs=5):
    model.to(device)
    best_acc = 0

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        correct, total = 0, 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} 训练中")

        for batch in progress_bar:
            input_ids, attention_mask, labels = (
                batch['input_ids'].to(device),
                batch['attention_mask'].to(device),
                batch['label'].to(device)
            )

            optimizer.zero_grad()

            # 🚀 开启自动混合精度训练
            with autocast():
                outputs = model(input_ids, attention_mask)
                loss = loss_fn(outputs, labels)

            # 🚀 采用梯度缩放优化 fp16 训练
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()
            preds = torch.argmax(outputs, dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

            progress_bar.set_postfix({"Loss": f"{total_loss / total:.4f}", "Acc": f"{correct / total:.4f}"})

        test_acc = evaluate_model(model, test_loader)
        print(f"🔍 Epoch {epoch+1} 测试集准确率: {test_acc:.4f}")

        if test_acc > best_acc:
            best_acc = test_acc
            torch.save(model.state_dict(), "best_bert_sentiment_model.pth")
            print(f"✅ 最优模型已保存！Test Acc: {best_acc:.4f}")
```
- **`autocast()`** 让计算使用 `fp16`，提高速度并减少显存占用。
- **`GradScaler`** 避免 `fp16` 计算时的精度问题。

---

### **✅ 方案 3：冻结 BERT 的底层权重**
**只训练分类层，而不训练 BERT 本体**（大大加快训练速度）
```python
class BERTSentimentClassifier(nn.Module):
    def __init__(self, pretrained_name='bert-base-chinese', num_classes=3):
        super(BERTSentimentClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_name)
        
        # 🚀 冻结 BERT 的所有参数
        for param in self.bert.parameters():
            param.requires_grad = False

        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        with torch.no_grad():  # 🚀 让 BERT 只进行推理，加速训练
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)

        cls_embedding = outputs.last_hidden_state[:, 0, :]
        output = self.dropout(cls_embedding)
        return self.fc(output)
```
- **BERT 参数不会更新，计算更快！**
- **只训练 `fc` 层，减少计算量！**

---

### **✅ 方案 4：使用 `pin_memory=True` 加速数据加载**
**在 `DataLoader` 里加 `pin_memory=True`** 提高数据传输速度：
```python
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)
```
- **`pin_memory=True`** 让数据直接传输到 GPU，提高吞吐量。
- **`num_workers=4`** 让数据加载更快（可以调整 `num_workers` 适配 CPU 性能）。

---

## **🚀 总结：如何加速训练**
| **优化方式** | **方法** | **提升效果** |
|-------------|---------|-------------|
| 🚀 **增大批量大小** | `batch_size=16/32` | **减少训练轮次** |
| ⚡ **混合精度训练** | `torch.cuda.amp` | **降低计算量，提高吞吐量** |
| 🔥 **冻结 BERT 参数** | `param.requires_grad = False` | **加快训练 3~5 倍** |
| 🚀 **多进程数据加载** | `num_workers=4` | **减少 I/O 负担** |
| 🏋️‍♂️ **数据传输优化** | `pin_memory=True` | **加快 CPU → GPU 传输** |

---

## **🌟 你的最终代码（优化版）**
```python
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)

class BERTSentimentClassifier(nn.Module):
    def __init__(self, pretrained_name='bert-base-chinese', num_classes=3):
        super(BERTSentimentClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_name)
        
        # 🚀 冻结 BERT 的所有参数
        for param in self.bert.parameters():
            param.requires_grad = False

        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        with torch.no_grad():  # 🚀 让 BERT 只进行推理，加速训练
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)

        cls_embedding = outputs.last_hidden_state[:, 0, :]
        output = self.dropout(cls_embedding)
        return self.fc(output)

scaler = GradScaler()

def train_model(model, train_loader, test_loader, optimizer, loss_fn, epochs=5):
    model.to(device)
    best_acc = 0

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        correct, total = 0, 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} 训练中")

        for batch in progress_bar:
            input_ids, attention_mask, labels = (
                batch['input_ids'].to(device),
                batch['attention_mask'].to(device),
                batch['label'].to(device)
            )

            optimizer.zero_grad()

            # 🚀 使用混合精度训练
            with autocast():
                outputs = model(input_ids, attention_mask)
                loss = loss_fn(outputs, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()
            preds = torch.argmax(outputs, dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

            progress_bar.set_postfix({"Loss": f"{total_loss / total:.4f}", "Acc": f"{correct / total:.4f}"})

        test_acc = evaluate_model(model, test_loader)
        print(f"🔍 Epoch {epoch+1} 测试集准确率: {test_acc:.4f}")

        if test_acc > best_acc:
            best_acc = test_acc
            torch.save(model.state_dict(), "best_bert_sentiment_model.pth")
            print(f"✅ 最优模型已保存！Test Acc: {best_acc:.4f}")
```

---

## **🔥 你可以试试看，现在训练应该能快 **2-5 倍** 了！** 🚀🚀🚀
