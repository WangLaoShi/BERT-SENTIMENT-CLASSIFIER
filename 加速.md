è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢çš„ä¸»è¦åŸå› å¯èƒ½æœ‰ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
1. **æ‰¹é‡å¤§å°ï¼ˆBatch Sizeï¼‰è¾ƒå°** ğŸ‹ï¸â€â™‚ï¸  
   - å½“å‰æ‰¹é‡å¤§å°æ˜¯ **8**ï¼Œå¯ä»¥å°è¯•å¢å¤§ï¼Œæ¯”å¦‚ **16 æˆ– 32**ï¼ˆå‰ææ˜¯æ˜¾å­˜å…è®¸ï¼‰ã€‚
  
2. **ä½¿ç”¨ `fp16` è¿›è¡Œæ··åˆç²¾åº¦è®­ç»ƒ** âš¡  
   - **è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰** å¯ä»¥åŠ é€Ÿè®¡ç®—å¹¶å‡å°‘æ˜¾å­˜å ç”¨ã€‚

3. **æ•°æ®åŠ è½½é€Ÿåº¦è¾ƒæ…¢** â³  
   - **ä½¿ç”¨ `num_workers` åŠ é€Ÿæ•°æ®åŠ è½½**ï¼Œé¿å… CPU è¯»å–æ•°æ®æˆä¸ºç“¶é¢ˆã€‚

4. **ä¼˜åŒ–è®¡ç®—æ•ˆç‡** ğŸ”¥  
   - **å†»ç»“ BERT çš„åº•å±‚æƒé‡**ï¼Œåªè®­ç»ƒä¸Šå±‚åˆ†ç±»å¤´ã€‚

---

## **ğŸ”¥ è§£å†³æ–¹æ¡ˆ**
æˆ‘ä¼š **ä¸€æ­¥æ­¥ä¼˜åŒ–ä½ çš„ä»£ç **ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦ï¼

### **âœ… æ–¹æ¡ˆ 1ï¼šå¢å¤§ Batch Size**
**ä¿®æ”¹ `train_loader` å’Œ `test_loader` çš„ `batch_size=16` æˆ– `batch_size=32`**ï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
```python
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)
```
- **å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥å°è¯• `batch_size=16`ï¼Œå¦åˆ™ `32` æ›´å¿«**ã€‚

---

### **âœ… æ–¹æ¡ˆ 2ï¼šå¯ç”¨ `fp16` æ··åˆç²¾åº¦è®­ç»ƒ**
ä½¿ç”¨ `torch.cuda.amp` è¿›è¡Œ **è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰**ï¼Œå‡å°‘ GPU è®¡ç®—é‡ï¼š
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

def train_model(model, train_loader, test_loader, optimizer, loss_fn, epochs=5):
    model.to(device)
    best_acc = 0

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        correct, total = 0, 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} è®­ç»ƒä¸­")

        for batch in progress_bar:
            input_ids, attention_mask, labels = (
                batch['input_ids'].to(device),
                batch['attention_mask'].to(device),
                batch['label'].to(device)
            )

            optimizer.zero_grad()

            # ğŸš€ å¼€å¯è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ
            with autocast():
                outputs = model(input_ids, attention_mask)
                loss = loss_fn(outputs, labels)

            # ğŸš€ é‡‡ç”¨æ¢¯åº¦ç¼©æ”¾ä¼˜åŒ– fp16 è®­ç»ƒ
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()
            preds = torch.argmax(outputs, dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

            progress_bar.set_postfix({"Loss": f"{total_loss / total:.4f}", "Acc": f"{correct / total:.4f}"})

        test_acc = evaluate_model(model, test_loader)
        print(f"ğŸ” Epoch {epoch+1} æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")

        if test_acc > best_acc:
            best_acc = test_acc
            torch.save(model.state_dict(), "best_bert_sentiment_model.pth")
            print(f"âœ… æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜ï¼Test Acc: {best_acc:.4f}")
```
- **`autocast()`** è®©è®¡ç®—ä½¿ç”¨ `fp16`ï¼Œæé«˜é€Ÿåº¦å¹¶å‡å°‘æ˜¾å­˜å ç”¨ã€‚
- **`GradScaler`** é¿å… `fp16` è®¡ç®—æ—¶çš„ç²¾åº¦é—®é¢˜ã€‚

---

### **âœ… æ–¹æ¡ˆ 3ï¼šå†»ç»“ BERT çš„åº•å±‚æƒé‡**
**åªè®­ç»ƒåˆ†ç±»å±‚ï¼Œè€Œä¸è®­ç»ƒ BERT æœ¬ä½“**ï¼ˆå¤§å¤§åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼‰
```python
class BERTSentimentClassifier(nn.Module):
    def __init__(self, pretrained_name='bert-base-chinese', num_classes=3):
        super(BERTSentimentClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_name)
        
        # ğŸš€ å†»ç»“ BERT çš„æ‰€æœ‰å‚æ•°
        for param in self.bert.parameters():
            param.requires_grad = False

        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        with torch.no_grad():  # ğŸš€ è®© BERT åªè¿›è¡Œæ¨ç†ï¼ŒåŠ é€Ÿè®­ç»ƒ
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)

        cls_embedding = outputs.last_hidden_state[:, 0, :]
        output = self.dropout(cls_embedding)
        return self.fc(output)
```
- **BERT å‚æ•°ä¸ä¼šæ›´æ–°ï¼Œè®¡ç®—æ›´å¿«ï¼**
- **åªè®­ç»ƒ `fc` å±‚ï¼Œå‡å°‘è®¡ç®—é‡ï¼**

---

### **âœ… æ–¹æ¡ˆ 4ï¼šä½¿ç”¨ `pin_memory=True` åŠ é€Ÿæ•°æ®åŠ è½½**
**åœ¨ `DataLoader` é‡ŒåŠ  `pin_memory=True`** æé«˜æ•°æ®ä¼ è¾“é€Ÿåº¦ï¼š
```python
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)
```
- **`pin_memory=True`** è®©æ•°æ®ç›´æ¥ä¼ è¾“åˆ° GPUï¼Œæé«˜ååé‡ã€‚
- **`num_workers=4`** è®©æ•°æ®åŠ è½½æ›´å¿«ï¼ˆå¯ä»¥è°ƒæ•´ `num_workers` é€‚é… CPU æ€§èƒ½ï¼‰ã€‚

---

## **ğŸš€ æ€»ç»“ï¼šå¦‚ä½•åŠ é€Ÿè®­ç»ƒ**
| **ä¼˜åŒ–æ–¹å¼** | **æ–¹æ³•** | **æå‡æ•ˆæœ** |
|-------------|---------|-------------|
| ğŸš€ **å¢å¤§æ‰¹é‡å¤§å°** | `batch_size=16/32` | **å‡å°‘è®­ç»ƒè½®æ¬¡** |
| âš¡ **æ··åˆç²¾åº¦è®­ç»ƒ** | `torch.cuda.amp` | **é™ä½è®¡ç®—é‡ï¼Œæé«˜ååé‡** |
| ğŸ”¥ **å†»ç»“ BERT å‚æ•°** | `param.requires_grad = False` | **åŠ å¿«è®­ç»ƒ 3~5 å€** |
| ğŸš€ **å¤šè¿›ç¨‹æ•°æ®åŠ è½½** | `num_workers=4` | **å‡å°‘ I/O è´Ÿæ‹…** |
| ğŸ‹ï¸â€â™‚ï¸ **æ•°æ®ä¼ è¾“ä¼˜åŒ–** | `pin_memory=True` | **åŠ å¿« CPU â†’ GPU ä¼ è¾“** |

---

## **ğŸŒŸ ä½ çš„æœ€ç»ˆä»£ç ï¼ˆä¼˜åŒ–ç‰ˆï¼‰**
```python
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)

class BERTSentimentClassifier(nn.Module):
    def __init__(self, pretrained_name='bert-base-chinese', num_classes=3):
        super(BERTSentimentClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_name)
        
        # ğŸš€ å†»ç»“ BERT çš„æ‰€æœ‰å‚æ•°
        for param in self.bert.parameters():
            param.requires_grad = False

        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        with torch.no_grad():  # ğŸš€ è®© BERT åªè¿›è¡Œæ¨ç†ï¼ŒåŠ é€Ÿè®­ç»ƒ
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)

        cls_embedding = outputs.last_hidden_state[:, 0, :]
        output = self.dropout(cls_embedding)
        return self.fc(output)

scaler = GradScaler()

def train_model(model, train_loader, test_loader, optimizer, loss_fn, epochs=5):
    model.to(device)
    best_acc = 0

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        correct, total = 0, 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} è®­ç»ƒä¸­")

        for batch in progress_bar:
            input_ids, attention_mask, labels = (
                batch['input_ids'].to(device),
                batch['attention_mask'].to(device),
                batch['label'].to(device)
            )

            optimizer.zero_grad()

            # ğŸš€ ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            with autocast():
                outputs = model(input_ids, attention_mask)
                loss = loss_fn(outputs, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()
            preds = torch.argmax(outputs, dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

            progress_bar.set_postfix({"Loss": f"{total_loss / total:.4f}", "Acc": f"{correct / total:.4f}"})

        test_acc = evaluate_model(model, test_loader)
        print(f"ğŸ” Epoch {epoch+1} æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")

        if test_acc > best_acc:
            best_acc = test_acc
            torch.save(model.state_dict(), "best_bert_sentiment_model.pth")
            print(f"âœ… æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜ï¼Test Acc: {best_acc:.4f}")
```

---

## **ğŸ”¥ ä½ å¯ä»¥è¯•è¯•çœ‹ï¼Œç°åœ¨è®­ç»ƒåº”è¯¥èƒ½å¿« **2-5 å€** äº†ï¼** ğŸš€ğŸš€ğŸš€
