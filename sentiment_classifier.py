import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import random
import csv
from transformers import BertTokenizer, BertModel
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

# é€‰æ‹©è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ============================
# 1. è¯»å–æ•°æ®å¹¶é¢„å¤„ç†
# ============================
class SentimentDataset(Dataset):
    def __init__(self, file_name, tokenizer, max_length=512):
        self.data = self.read_file(file_name)
        self.tokenizer = tokenizer
        self.max_length = max_length

    def read_file(self, file_name):
        with open(file_name, 'r', encoding='UTF-8') as f:
            reader = csv.reader(f)
            data = [[line[0], int(line[1])] for line in reader if len(line) >= 2 and len(line[0]) > 0]
        random.shuffle(data)  # æ‰“ä¹±æ•°æ®
        return pd.DataFrame(data, columns=["text", "label"])

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data.iloc[idx, 0]
        label = self.data.iloc[idx, 1]

        encoding = self.tokenizer(
            text, 
            padding='max_length', 
            truncation=True, 
            max_length=self.max_length, 
            return_tensors="pt"
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'label': torch.tensor(label, dtype=torch.long)
        }

# ============================
# 2. å®šä¹‰ BERT åˆ†ç±»æ¨¡å‹
# ============================
class BERTSentimentClassifier(nn.Module):
    def __init__(self, pretrained_name='bert-base-chinese', num_classes=3):
        super(BERTSentimentClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_name)
        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_embedding = outputs.last_hidden_state[:, 0, :]  # å– [CLS] ä½ç½®çš„å‘é‡
        output = self.dropout(cls_embedding)
        return self.fc(output)

# ============================
# 3. è®­ç»ƒå‡½æ•°
# ============================
def train_model(model, train_loader, test_loader, optimizer, loss_fn, epochs=5):
    model.to(device)
    best_acc = 0

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        correct, total = 0, 0

        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} è®­ç»ƒä¸­")

        for batch in progress_bar:
            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)
            
            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)
            loss = loss_fn(outputs, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            preds = torch.argmax(outputs, dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

            progress_bar.set_postfix({"Loss": f"{total_loss / total:.4f}", "Acc": f"{correct / total:.4f}"})

        # è®¡ç®—æµ‹è¯•é›†å‡†ç¡®ç‡
        test_acc = evaluate_model(model, test_loader)
        print(f"ğŸ” Epoch {epoch+1} æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        if test_acc > best_acc:
            best_acc = test_acc
            torch.save(model.state_dict(), "best_bert_sentiment_model.pth")
            print(f"âœ… æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜ï¼Test Acc: {best_acc:.4f}")

# ============================
# 4. è¯„ä¼°å‡½æ•°
# ============================
def evaluate_model(model, test_loader):
    model.eval()
    correct, total = 0, 0

    with torch.no_grad():
        for batch in test_loader:
            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)

            outputs = model(input_ids, attention_mask)
            preds = torch.argmax(outputs, dim=1)

            correct += (preds == labels).sum().item()
            total += labels.size(0)

    return correct / total

# ============================
# 5. è¿è¡Œè®­ç»ƒå’Œè¯„ä¼°
# ============================
if __name__ == "__main__":
    # åŠ è½½ tokenizer
    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')

    # è¯»å–æ•°æ®
    dataset = SentimentDataset("./file/comments.csv", tokenizer)
    train_size = int(0.8 * len(dataset))
    test_size = len(dataset) - train_size
    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

    # åˆå§‹åŒ–æ¨¡å‹
    model = BERTSentimentClassifier()

    # å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=1e-5)
    loss_fn = nn.CrossEntropyLoss()

    # è®­ç»ƒæ¨¡å‹
    train_model(model, train_loader, test_loader, optimizer, loss_fn, epochs=5)

    # æµ‹è¯•æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load("best_bert_sentiment_model.pth"))
    final_acc = evaluate_model(model, test_loader)
    print(f"ğŸ† æœ€ç»ˆæµ‹è¯•é›†å‡†ç¡®ç‡: {final_acc:.4f}")
