import csv
import pandas as pd
import random
import torch
from transformers import BertTokenizer, BertModel
from torch import nn
from tqdm import tqdm

"""
è¯»å–è¯„è®ºæ–‡ä»¶çš„è¯„è®ºä¿¡æ¯
"""
def read_file(file_name):
    comments_data = None
    # è¯»å–è¯„è®ºä¿¡æ¯
    with open(file_name, 'r', encoding='UTF-8') as f:
        reader = csv.reader(f)
        comments_data = [[line[0], int(line[1])] for line in reader]                      # è¯»å–è¯„è®ºæ•°æ®å’Œå¯¹åº”çš„æ ‡ç­¾ä¿¡æ¯
    
    # æ‰“ä¹±æ•°æ®é›†
    random.shuffle(comments_data)
    data = pd.DataFrame(comments_data)
    same_sentence_num = data.duplicated().sum()                                           # ç»Ÿè®¡é‡å¤çš„è¯„è®ºå†…å®¹ä¸ªæ•°
    
    if same_sentence_num > 0:
        data = data.drop_duplicates()                                                     # åˆ é™¤é‡å¤çš„æ ·æœ¬ä¿¡æ¯
    
    f.close()
    
    return data


comments_data = read_file('./file/comments.csv')
print(len(comments_data))


# åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†ï¼Œå¹¶å°†pandasæ•°æ®ç±»å‹è½¬åŒ–ä¸ºåˆ—è¡¨ç±»å‹
train_comments, train_labels = list(comments_data[: split_line][0]), list(comments_data[: split_line][1])
test_comments, test_labels = list(comments_data[split_line:][0]), list(comments_data[split_line:][1])

print(len(train_comments),len(train_labels), len(test_comments), len(test_labels))

from transformers import AutoTokenizer, AutoModelForSequenceClassification

"""
Step2: å®šä¹‰BERTClassifieråˆ†ç±»å™¨æ¨¡å‹
"""
class BERTClassifier(nn.Module):

    # åˆå§‹åŒ–åŠ è½½ bert-base-chinese åŸå‹ï¼Œå³Bertä¸­çš„Bert-Baseæ¨¡å‹
    def __init__(self, output_dim, pretrained_name='bert-base-chinese'):

        super(BERTClassifier, self).__init__()
        
        # Step 2: åŠ è½½æ¨¡å‹
        model = AutoModelForSequenceClassification.from_pretrained(pretrained_name,local_files_only=True)

        # å°†æ¨¡å‹ç§»åŠ¨åˆ° GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)

        # å®šä¹‰ Bert æ¨¡å‹
        # self.bert = BertModel.from_pretrained(pretrained_name)
        self.bert = model

        # å¤–æ¥å…¨è¿æ¥å±‚
        self.mlp = nn.Linear(768, output_dim)


    def forward(self, tokens_X):

        # å¾—åˆ°æœ€åä¸€å±‚çš„ '<cls>' ä¿¡æ¯ï¼Œ å…¶æ ‡å¿—å…¨éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯
        res = self.bert(**tokens_X)

        # res[1]ä»£è¡¨åºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯'<cls>'ï¼Œå¤–æ¥å…¨è¿æ¥å±‚ï¼Œè¿›è¡Œæƒ…æ„Ÿåˆ†æ 
        # return self.mlp(res[1])
#         ğŸ“Œ ä¸ºä»€ä¹ˆä¼šæŠ¥é”™ï¼Ÿ
#         self.bert(**tokens_X) è¿”å›çš„æ˜¯ä¸€ä¸ª BaseModelOutputWithPoolingAndCrossAttentions å¯¹è±¡ï¼Œä¸ä¸€å®šæœ‰ res[1]ã€‚
#         ä¸åŒ BERT å˜ä½“ï¼Œè¿”å›çš„ res ç»“æ„å¯èƒ½ä¸åŒï¼Œæ­£ç¡®çš„æå–æ–¹å¼å–å†³äº BERT ç‰ˆæœ¬ï¼š

#         res[0]ï¼šæœ€åä¸€å±‚éšè—çŠ¶æ€ï¼ˆå½¢çŠ¶ [batch_size, seq_length, hidden_dim]ï¼‰
#         res[1]ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰ï¼špooler_outputï¼Œå³ [CLS] token çš„åµŒå…¥ï¼ˆé€šå¸¸ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼‰
#         å¦‚æœ res[1] ä¸å­˜åœ¨ï¼Œä½ åº”è¯¥ç”¨ res.pooler_outputï¼Œæˆ–è€… res[0][:, 0, :] ä»£æ›¿ã€‚
        # ç›´æ¥ä½¿ç”¨ pooler_output ä½œä¸ºç‰¹å¾
        # return self.mlp(res.pooler_output)
        return res.logits



"""
è¯„ä¼°å‡½æ•°ï¼Œç”¨ä»¥è¯„ä¼°æ•°æ®é›†åœ¨ç¥ç»ç½‘ç»œä¸‹çš„ç²¾ç¡®åº¦
"""
def evaluate(net, comments_data, labels_data):
    
    sum_correct, i = 0, 0
    
    while i <= len(comments_data):
        
        comments = comments_data[i: min(i + 8, len(comments_data))]
        
        tokens_X = tokenizer(comments,
                             padding=True,
                             truncation=True,
                             max_length=512,  # é™åˆ¶æœ€å¤§é•¿åº¦
                             return_tensors='pt').to(device=device)

        res = net(tokens_X)                                          # è·å¾—åˆ°é¢„æµ‹ç»“æœ

        y = torch.tensor(labels_data[i: min(i + 8, len(comments_data))]).reshape(-1).to(device=device)

        sum_correct += (res.argmax(axis=1) == y).sum()              # ç´¯åŠ é¢„æµ‹æ­£ç¡®çš„ç»“æœ
        i += 8

    return sum_correct/len(comments_data)                           # è¿”å›(æ€»æ­£ç¡®ç»“æœ/æ‰€æœ‰æ ·æœ¬)ï¼Œç²¾ç¡®ç‡


"""
è®­ç»ƒbert_classifieråˆ†ç±»å™¨

"""
def train_bert_classifier(net, tokenizer, loss, optimizer, train_comments, train_labels, test_comments, test_labels, device, epochs):
    
    max_acc = 0.5                                 # åˆå§‹åŒ–æ¨¡å‹æœ€å¤§ç²¾åº¦ä¸º0.5
    
    # ç´¯è®¡è®­ç»ƒ 18 ä¸‡æ¡æ•°æ® epochs æ¬¡ï¼Œä¼˜åŒ–æ¨¡å‹
    for epoch in tqdm(range(epochs)):
        
        i, sum_loss = 0, 0                           # æ¯æ¬¡å¼€å§‹è®­ç»ƒæ—¶ï¼Œ i ä¸º0 è¡¨ç¤ºä»ç¬¬ä¸€æ¡æ•°æ®å¼€å§‹è®­ç»ƒ
        
        # è®¡ç®—è®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„ç²¾åº¦
        train_acc = evaluate(net, train_comments, train_labels)
        test_acc = evaluate(net, test_comments, test_labels)
        
        # è¾“å‡ºç²¾åº¦
        print('\n--epoch', epoch, '\t--loss:', sum_loss / (len(train_comments) / 8), '\t--train_acc:', train_acc, '\t--test_acc', test_acc)
        
        
        
        # å¦‚æœæµ‹è¯•é›†ç²¾åº¦ å¤§äº ä¹‹å‰ä¿å­˜çš„æœ€å¤§ç²¾åº¦ï¼Œä¿å­˜æ¨¡å‹å‚æ•°ï¼Œå¹¶é‡è®¾æœ€å¤§å€¼
        if test_acc > max_acc:
            
            # æ›´æ–°å†å²æœ€å¤§ç²¾ç¡®åº¦
            max_acc = test_acc
            
            # ä¿å­˜æ¨¡å‹
            torch.save(net.state_dict(), 'bert.parameters')
        
        
        # å¼€å§‹è®­ç»ƒæ¨¡å‹
        while i < len(comments_data):
            comments = train_comments[i: min(i+8, len(train_comments))]             # æ‰¹é‡è®­ç»ƒï¼Œæ¯æ¬¡è®­ç»ƒ8æ¡æ ·æœ¬æ•°æ®

            # é€šè¿‡ tokenizer æ•°æ®åŒ–è¾“å…¥çš„è¯„è®ºè¯­å¥ä¿¡æ¯ï¼Œå‡†å¤‡è¾“å…¥bertåˆ†ç±»å™¨
            # è¾“å…¥çš„8ä¸ªè¯„è®ºè¯­å¥é•¿åº¦å¾ˆå¯èƒ½ä¸ä¸€è‡´ï¼Œè¿™æ—¶å–é•¿åº¦ä¸ºæœ€é•¿çš„é‚£ä¸ªå¥å­ï¼Œpadding=Trueä»£è¡¨å¯¹çŸ­å¥å­è¿›è¡Œå¡«å……æ“ä½œ
            # å½“è¾“å…¥çš„æŸä¸ªå¥å­è¿‡é•¿æ—¶ï¼Œä½¿ç”¨truncation=Trueè¿›è¡Œæˆªæ–­æ“ä½œ
            # return_tensors='pt' ä»£è¡¨è¿”å›çš„æ•°æ®ç±»å‹ä¸º python çš„ torch ç±»å‹
            tokens_X = tokenizer(comments, padding=True, truncation=True, return_tensors='pt').to(device=device)

            # å°†æ•°æ®è¾“å…¥åˆ°bertåˆ†ç±»å™¨æ¨¡å‹ä¸­ï¼Œè·å¾—ç»“æœ
            res = net(tokens_X)

            # æ‰¹é‡è·å–å®é™…ç»“æœä¿¡æ¯
            y = torch.tensor(train_labels[i: min(i+8, len(train_comments))]).reshape(-1).to(device=device)

            optimizer.zero_grad()                  # æ¸…ç©ºæ¢¯åº¦
            l = loss(res, y)                       # è®¡ç®—æŸå¤±
            l.backward()                           # åå‘ä¼ æ’­
            optimizer.step()                      # æ›´æ–°æ¢¯åº¦

            sum_loss += l.detach()                # ç´¯åŠ æŸå¤±
            i += 8                                # æ ·æœ¬ä¸‹æ ‡ç´¯åŠ 


from d2l import torch as d2l

device = d2l.try_gpu()                                  # è·å–GPU

net = BERTClassifier(output_dim=3)                      # BERTClassifieråˆ†ç±»å™¨ï¼Œå› ä¸ºæœ€ç»ˆç»“æœä¸º3åˆ†ç±»ï¼Œæ‰€ä»¥è¾“å‡ºç»´åº¦ä¸º3ï¼Œä»£è¡¨æ¦‚ç‡åˆ†å¸ƒ
net = net.to(device)


# å®šä¹‰tokenizerå¯¹è±¡ï¼Œç”¨äºå°†è¯„è®ºè¯­å¥è½¬åŒ–ä¸ºBertModelçš„è¾“å…¥ä¿¡æ¯
# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')

from modelscope.models import Model
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese",local_files_only=True)

model = AutoModelForSequenceClassification.from_pretrained("bert-base-chinese",local_files_only=True)

# Step 3: ç§»åŠ¨åˆ° GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

print("æ¨¡å‹å’Œ Tokenizer åŠ è½½å®Œæˆï¼")


loss = nn.CrossEntropyLoss()                            # æŸå¤±å‡½æ•°
optimizer = torch.optim.SGD(net.parameters(), lr=1e-4)      # å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•


train_bert_classifier(net,
                      tokenizer,
                      loss,
                      optimizer,
                      train_comments,
                      train_labels,
                      test_comments,
                      test_labels,
                      device,
                      20)